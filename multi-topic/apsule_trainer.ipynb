{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.加载各种库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pandas as pd\n",
    "from capsule import *\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.导入停用词、词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 635793 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# not enable in windows\n",
    "\n",
    "# jieba.enable_parallel(4)\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "remove_stop_words = False\n",
    "\n",
    "train_file = '/home/carnd/BDCI/multi-topic/train.csv'\n",
    "test_file = '/home/carnd/BDCI/multi-topic/test_public.csv'\n",
    "\n",
    "# load stopwords\n",
    "f = open('/home/carnd/BDCI/multi-topic/stopwords.txt','r', encoding='utf-8')\n",
    "stop_words = []\n",
    "for line in f.readlines():\n",
    "    stop_words.append(line.strip())\n",
    "stop_words = set(stop_words)\n",
    "\n",
    "# load Glove Vectors\n",
    "embeddings_index = {}\n",
    "EMBEDDING_DIM = 300\n",
    "embfile = '/home/carnd/BDCI/multi-topic/sgns.baidubaike.bigram-char'\n",
    "\n",
    "#从预训练的词向量中构建词向量\n",
    "with open(embfile, encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        values = line.split()\n",
    "        words = values[:-EMBEDDING_DIM]\n",
    "        word = ''.join(words)\n",
    "        try:\n",
    "            coefs = np.asarray(values[-EMBEDDING_DIM:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        except:\n",
    "            pass\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.数据预处理\n",
    "#### 3.1\n",
    "    - 去除停用词\n",
    "    - 将数据用dict类型表示\n",
    "        - key = 评论\n",
    "        - value = {'主题+情感值'}\n",
    "    - 构造feature集和label集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#构建训练集\n",
    "train_df = pd.read_csv(train_file, encoding='utf-8')\n",
    "test_df = pd.read_csv(test_file, encoding='utf-8')\n",
    "train_df['label'] = train_df['subject'].str.cat(train_df['sentiment_value'].astype(str))\n",
    "\n",
    "\n",
    "#去除训练集和测试集的停用词\n",
    "if remove_stop_words:\n",
    "    train_df['content'] = train_df.content.map(\n",
    "        lambda x: ''.join([e for e in x.strip().split() if e not in stop_words]))\n",
    "    test_df['content'] = test_df.content.map(\n",
    "        lambda x: ''.join([e for e in x.strip().split() if e not in stop_words]))\n",
    "else:\n",
    "    train_df['content'] = train_df.content.map(lambda x: ''.join(x.strip().split()))\n",
    "    test_df['content'] = test_df.content.map(lambda x: ''.join(x.strip().split()))\n",
    "    \n",
    "#将数据用dict类型表示   \n",
    "train_dict = {}\n",
    "for ind, row in train_df.iterrows():\n",
    "    content, label = row['content'], row['label']\n",
    "    if train_dict.get(content) is None:\n",
    "        train_dict[content] = set([label])\n",
    "    else:\n",
    "        train_dict[content].add(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'动力1', '油耗1'}\n"
     ]
    }
   ],
   "source": [
    "print(train_dict['一直92，偶尔出去了不了解当地油品加95(97)。5万公里从没遇到问题，省油，动力也充足，加95也没感觉有啥不同。'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conts = []\n",
    "labels = []\n",
    "\n",
    "#构造feature集和label集  \n",
    "for k, v in train_dict.items():\n",
    "    conts.append(k)\n",
    "    labels.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['楼主好执着，终于入手xt。我开了半年多感觉动力还是差点，40万以内买suv动力也就这意思了。', '新一代鹰眼2.0的会不会有，还有后排出风口', '确实可以从中控看，挺好用的', '车身刚性应该是提高了，现款的森总感觉薄薄的，开关门没有德系车的厚重感', '加个尾排，会对车性能有影响？？', '你测过百公里加速时间吗。我反复测s#模式，最好一次7.6秒，一般都是在8秒左右。', '是否有保留2.0L发动机？', '换回原厂cd用手机导航', '性能车就不能谈上，2.5后劲不是很好', '这个挺好的啊，我把导航换了，换成了这个']\n",
      "[{'动力0'}, {'空间0'}, {'配置0'}, {'外观-1'}, {'动力0', '操控0'}, {'动力0'}, {'动力0'}, {'配置0'}, {'动力0', '操控0'}, {'配置0'}]\n"
     ]
    }
   ],
   "source": [
    "print( conts[4090:4100])\n",
    "print( labels[4090:4100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.数据预处理\n",
    "#### 3.2\n",
    "    - 对数据标签进行one-hot编码\n",
    "      这里的label数量是：10种主题*3种情感 = 30个\n",
    "    - 调用jieba分词对评论分词\n",
    "    - 构建词典\n",
    "    - 构建词汇的embedding矩阵\n",
    "    - 对数据进行padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19881\n"
     ]
    }
   ],
   "source": [
    "#对数据标签进行one-hot编码\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train = mlb.fit_transform(labels)\n",
    "\n",
    "#对训练集和测试集进行jieba分词\n",
    "content_list = [jieba.lcut(str(c)) for c in conts]\n",
    "test_content_list = [jieba.lcut(c) for c in test_df.content.astype(str).values]\n",
    "\n",
    "#构建词典\n",
    "word_set = set([word for row in list(content_list) + list(test_content_list) for word in row])\n",
    "print(len(word_set))\n",
    "\n",
    "#将评论转化为单词的index矩阵\n",
    "word2index = {w: i + 1 for i, w in enumerate(word_set)}\n",
    "seqs = [[word2index[w] for w in l] for l in content_list]\n",
    "seqs_dev = [[word2index[w] for w in l] for l in test_content_list]\n",
    "embedding_matrix = np.zeros((len(word2index) + 1, EMBEDDING_DIM))\n",
    "\n",
    "#构建词汇的embedding矩阵\n",
    "for word, i in word2index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "max_features = len(word_set) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_padding_data(maxlen=100):\n",
    "\n",
    "    x_train = sequence.pad_sequences(seqs, maxlen=maxlen)\n",
    "    x_dev = sequence.pad_sequences(seqs_dev, maxlen=maxlen)\n",
    "    return x_train, x_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.构建网络模型\n",
    "    - 输入层\n",
    "    - embedding层 -> Dropout层 -> 双向RNN -> capsule层 -> Flatten层 -> Dropout层 -> 全连接层\n",
    "    - 输出层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_capsule_model():\n",
    "\n",
    "    input1 = Input(shape=(maxlen,))\n",
    "    embed_layer = Embedding(len(word2index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=maxlen,\n",
    "                            trainable=False)(input1)\n",
    "\n",
    "    embed_layer = SpatialDropout1D(rate_drop_dense)(embed_layer)\n",
    "\n",
    "\n",
    "    '''\n",
    "    x = Bidirectional(\n",
    "        GRU(gru_len, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, return_sequences=True))(\n",
    "        embed_layer)    \n",
    "    '''\n",
    "    x = Bidirectional(\n",
    "        GRU(gru_len, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, return_sequences=True))(\n",
    "        embed_layer)\n",
    "\n",
    "    capsule = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings,\n",
    "                      share_weights=True)(x)\n",
    "\n",
    "    # output_capsule = Lambda(lambda x: K.sqrt(K.sum(K.square(x), 2)))(capsule)\n",
    "\n",
    "    capsule = Flatten()(capsule)\n",
    "    capsule = Dropout(dropout_p)(capsule)\n",
    "\n",
    "    output = Dense(30, activation='sigmoid')(capsule)\n",
    "\n",
    "    model = Model(inputs=input1, outputs=output)\n",
    "\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8290, 100) (2364, 100) (8290, 30)\n",
      "Epoch 1/1\n",
      "8290/8290 [==============================] - 139s 17ms/step - loss: 0.2274 - acc: 0.9534\n",
      "Epoch 1/1\n",
      " 128/8290 [..............................] - ETA: 8:15 - loss: 0.6081 - acc: 0.7099 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-7f9667f9d623>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_capsule_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mbatchsize_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mfirst_model_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mbatchsize_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/BDCI/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/BDCI/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/BDCI/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/BDCI/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/BDCI/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "maxlen = 100\n",
    "X_train, X_dev = get_padding_data(maxlen)\n",
    "print(X_train.shape, X_dev.shape, y_train.shape)\n",
    "\n",
    "\n",
    "# train model and find params\n",
    "\n",
    "# model = get_capsule_model()\n",
    "\n",
    "# batch_size = 30\n",
    "\n",
    "# epochs = 50\n",
    "\n",
    "# file_path = \"weights_base.best.hdf5\"\n",
    "\n",
    "# checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=2)\n",
    "\n",
    "# callbacks_list = [checkpoint, early]  # early\n",
    "\n",
    "# model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list)\n",
    "\n",
    "batchsize_train = 64\n",
    "epochs = 15\n",
    "batchsize_test = 1024\n",
    "first_model_results = []\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    model = get_capsule_model()\n",
    "    model.fit(X_train, y_train, batch_size= batchsize_train, epochs=epochs)\n",
    "    first_model_results.append(model.predict(X_dev, batch_size= batchsize_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2364, 30)\n"
     ]
    }
   ],
   "source": [
    "print( np.shape(first_model_results) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.预测\n",
    "    - 如果预测最大值 < 0.5 则取最大值对应的分类做单分类\n",
    "    - 如果预测最大值 > 0.5 则按照预测值取整做分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred4 = np.average(first_model_results, axis=0)\n",
    "tmp = [[i for i in row] for row in pred4]\n",
    "\n",
    "\n",
    "''' 如果预测最大值 < 0.5 则取最大值对应的分类做单分类\n",
    "    如果预测最大值 > 0.5 则按照预测值取整做分类'''\n",
    "for i, v in enumerate(tmp):\n",
    "    if max(v) < 0.5:\n",
    "        max_val = max(v)\n",
    "        tmp[i] = [1 if j == max_val else 0 for j in v]\n",
    "    else:\n",
    "        tmp[i] = [int(round(j)) for j in v]\n",
    "\n",
    "tmp = np.asanyarray(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.将数据还原为标准格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = mlb.inverse_transform(tmp)\n",
    "cids = []\n",
    "subjs = []\n",
    "sent_vals = []\n",
    "\n",
    "for c, r in zip(test_df.content_id, res):\n",
    "    for t in r:\n",
    "        if '-' in t:\n",
    "            sent_val = -1\n",
    "            subj = t[:-2]\n",
    "        else:\n",
    "            sent_val = int(t[-1])\n",
    "            subj = t[:-1]\n",
    "        cids.append(c)\n",
    "        subjs.append(subj)\n",
    "        sent_vals.append(sent_val)\n",
    "        \n",
    "\n",
    "res_df = pd.DataFrame({'content_id': cids, 'subject': subjs, 'sentiment_value': sent_vals,\n",
    "\n",
    "                       'sentiment_word': ['一般' for i in range(len(cids))]})\n",
    "\n",
    "\n",
    "\n",
    "columns = ['content_id', 'subject', 'sentiment_value', 'sentiment_word']\n",
    "\n",
    "res_df = res_df.reindex(columns=columns)\n",
    "\n",
    "res_df.to_csv('submit_capsule_word.csv', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:BDCI]",
   "language": "python",
   "name": "conda-env-BDCI-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
