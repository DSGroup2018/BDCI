{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfTransformer,TfidfVectorizer\n",
    " \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.metrics import f1_score,accuracy_score,classification_report\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "import pickle as pk\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('/home/carnd/BDCI/baseline/stopwords.txt','r', encoding='utf-8')\n",
    "#stopwords_voacb = f.readline()\n",
    "stopwords = []\n",
    "for line in f.readlines():\n",
    "    stopwords.append(line.strip())\n",
    "#print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/home/carnd/BDCI/baseline/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    train = pd.read_csv(path + 'train.csv')\n",
    "    test = pd.read_csv(path + 'test_public.csv')\n",
    " \n",
    "    #将现有的数据随机打乱构成新的训练集，并在后面加上测试集\n",
    "    train = train.sample(frac=1)\n",
    "    train = train.reset_index(drop=True)\n",
    "    data = pd.concat([train, test])\n",
    "    \n",
    "    #将lable用编码\n",
    "    lbl =  LabelEncoder()\n",
    "    lbl.fit(train['subject'])\n",
    "    nb_classes = len(list(lbl.classes_))\n",
    "    pk.dump(lbl, open('label_encoder.sav','wb'))\n",
    "    \n",
    "    #将数据的主题全部用数字表示\n",
    "    subject = lbl.transform(train['subject'])\n",
    " \n",
    "    #构建情感值label\n",
    "    y = []\n",
    "    for i in list(train['sentiment_value']):\n",
    "        y.append(i)\n",
    " \n",
    "    #构建主题label\n",
    "    y1= []\n",
    "    for i in subject:\n",
    "        y1.append(i)\n",
    " \n",
    "    print(np.array(y).reshape(-1,1)[:,0])\n",
    "    return data,train.shape[0],np.array(y).reshape(-1,1)[:,0],test['content_id'],np.array(y1).reshape(-1,1)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#对给定的字符串data用jieba分词\n",
    "def processing_data(data):\n",
    "    word = jieba.cut(data)\n",
    "    return ' '.join(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pre_process():\n",
    "  \n",
    "    data,nrw_train,y,test_id,y1 = get_data()\n",
    " \n",
    "    #data['cut_commen']是分词之后的评论集\n",
    "    data['cut_comment'] = data['content'].map(processing_data)\n",
    "    \n",
    "    #构建特征矩阵\n",
    "    print('TfidfVectorizer')\n",
    "    tf = TfidfVectorizer(ngram_range=(1,3),\n",
    "                         token_pattern=r\"(?u)\\b\\w+\\b\", \n",
    "                         analyzer='char',\n",
    "                         stop_words=stopwords\n",
    "                         )\n",
    "    discuss_tf = tf.fit_transform(data['cut_comment'])\n",
    " \n",
    "    print('HashingVectorizer')\n",
    "    ha = HashingVectorizer(ngram_range=(1,2),\n",
    "                           token_pattern=r\"(?u)\\b\\w+\\b\",\n",
    "                           lowercase=False,\n",
    "                           stop_words=stopwords\n",
    "                           )\n",
    "    discuss_ha = ha.fit_transform(data['cut_comment'])\n",
    " \n",
    "    data = hstack((discuss_tf,discuss_ha)).tocsr()\n",
    " \n",
    "    return data[:nrw_train],data[nrw_train:],y,test_id,y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/BDCI/lib/python3.6/site-packages/ipykernel/__main__.py:8: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.905 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer\n",
      "HashingVectorizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/BDCI/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:286: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['一', '元', '吨', '数', '日', '末'] not in stop_words.\n",
      "  sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "X,test,y,test_id,y1= pre_process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 10\n",
    "kf = StratifiedKFold(n_splits=N, random_state=2018).split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#clf = LogisticRegression(C=0.5)\n",
    "clf = svm.LinearSVC(loss='hinge', tol=1e-4, C=0.4)\n",
    "clf_1 = svm.LinearSVC(loss='hinge', tol=1e-4, C=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_oofp = np.zeros_like(y, dtype='float64')\n",
    "y_train_oofp1 = np.zeros_like(y, dtype='float64')\n",
    " \n",
    "y_test_oofp = np.zeros((test.shape[0], N))\n",
    "y_test_oofp_1 = np.zeros((test.shape[0], N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def micro_avg_f1(y_true, y_pred):\n",
    "    return metrics.f1_score(y_true, y_pred, average='micro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_value_f1:0.719880\n",
      "topic_f1:0.688755\n",
      "sentiment_value_f1:0.722613\n",
      "topic_f1:0.716583\n",
      "sentiment_value_f1:0.730653\n",
      "topic_f1:0.731658\n",
      "sentiment_value_f1:0.715578\n",
      "topic_f1:0.723618\n",
      "sentiment_value_f1:0.722613\n",
      "topic_f1:0.702513\n",
      "sentiment_value_f1:0.713568\n",
      "topic_f1:0.693467\n",
      "sentiment_value_f1:0.731388\n",
      "topic_f1:0.706237\n",
      "sentiment_value_f1:0.735412\n",
      "topic_f1:0.703219\n",
      "sentiment_value_f1:0.726358\n",
      "topic_f1:0.713280\n",
      "sentiment_value_f1:0.726358\n",
      "topic_f1:0.702213\n",
      "0.7244421746316123\n",
      "0.7081543914249355\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "vcc = 0\n",
    "for i ,(train_fold,test_fold) in enumerate(kf):\n",
    "    X_train, X_validate, label_train, label_validate,  label_1_train, label_1_validate,= X[train_fold, :], X[test_fold, :], y[train_fold], y[test_fold], y1[train_fold], y1[test_fold]\n",
    "    clf.fit(X_train, label_train)\n",
    "    \n",
    "    val_ = clf.predict(X_validate)\n",
    "    y_train_oofp[test_fold] = val_\n",
    "    print('sentiment_value_f1:%f' % micro_avg_f1(label_validate, val_))\n",
    "    acc += micro_avg_f1(label_validate, val_)\n",
    "    result = clf.predict(test)\n",
    "    y_test_oofp[:, i] = result\n",
    " \n",
    "\n",
    "    clf_1.fit(X_train, label_1_train)\n",
    "    val_1 = clf_1.predict(X_validate)\n",
    "    y_train_oofp1[test_fold] = val_1\n",
    "    \n",
    "    print('topic_f1:%f' % micro_avg_f1(label_1_validate, val_1))\n",
    "    vcc += micro_avg_f1(label_1_validate, val_1)\n",
    "    result = clf_1.predict(test)\n",
    "    y_test_oofp_1[:, i] = result\n",
    " \n",
    "print(acc/N)\n",
    "print(vcc/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lbl = pk.load(open('label_encoder.sav','rb'))\n",
    "res_2 = []\n",
    "for i in range(y_test_oofp_1.shape[0]):\n",
    "    tmp = []\n",
    "    for j in range(N):\n",
    "        tmp.append(int(y_test_oofp_1[i][j]))\n",
    "    word_counts = Counter(tmp)\n",
    "    yes = word_counts.most_common(1)\n",
    "    res_2.append(lbl.inverse_transform([yes[0][0]])[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(y_test_oofp.shape[0]):\n",
    "    tmp = []\n",
    "    for j in range(N):\n",
    "        tmp.append(y_test_oofp[i][j])\n",
    "    res.append(max(set(tmp), key=tmp.count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2364\n"
     ]
    }
   ],
   "source": [
    "print(len(res))\n",
    "result = pd.DataFrame()\n",
    "result['content_id'] = list(test_id)\n",
    " \n",
    "result['subject'] = list(res_2)\n",
    "result['subject'] = result['subject']\n",
    " \n",
    "result['sentiment_value'] = list(res)\n",
    "result['sentiment_value'] = result['sentiment_value'].astype(int)\n",
    " \n",
    "result['sentiment_word'] = ''\n",
    "result.to_csv('submit.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:BDCI]",
   "language": "python",
   "name": "conda-env-BDCI-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
